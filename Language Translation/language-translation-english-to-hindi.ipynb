{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T12:17:20.800451Z","iopub.status.busy":"2024-06-24T12:17:20.799802Z","iopub.status.idle":"2024-06-24T12:17:20.812584Z","shell.execute_reply":"2024-06-24T12:17:20.811641Z","shell.execute_reply.started":"2024-06-24T12:17:20.800389Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Python version\n","3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n","Version info.\n","sys.version_info(major=3, minor=10, micro=13, releaselevel='final', serial=0)\n"]}],"source":["import sys\n","print(\"Python version\")\n","print(sys.version)\n","print(\"Version info.\")\n","print(sys.version_info)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:56.725440Z","iopub.status.busy":"2024-06-24T16:51:56.724703Z","iopub.status.idle":"2024-06-24T16:51:57.015075Z","shell.execute_reply":"2024-06-24T16:51:57.013997Z","shell.execute_reply.started":"2024-06-24T16:51:56.725407Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensorflow version 2.15.0\n","CUDA Version: True\n","GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","GPU Device Name: /device:GPU:0\n"]}],"source":["import tensorflow as tf\n","\n","print(\"tensorflow version\", tf.__version__)\n","print(\"CUDA Version:\", tf.test.is_built_with_cuda())\n","print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n","print(\"GPU Device Name:\", tf.test.gpu_device_name())\n"]},{"cell_type":"markdown","metadata":{},"source":["Dataset: https://www.kaggle.com/datasets/aiswaryaramachandran/hindienglish-corpora"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:57.253320Z","iopub.status.busy":"2024-06-24T16:51:57.252474Z","iopub.status.idle":"2024-06-24T16:51:57.866460Z","shell.execute_reply":"2024-06-24T16:51:57.865551Z","shell.execute_reply.started":"2024-06-24T16:51:57.253293Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/Hindi_English_Truncated_Corpus.csv\n"]}],"source":["import os\n","import string\n","import numpy as np\n","import pandas as pd\n","from string import digits\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","import re\n","import logging\n","import tensorflow as tf\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","logging.getLogger('tensorflow').setLevel(logging.FATAL)\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","import unicodedata\n","import io\n","import time\n","import warnings\n","import sys\n","\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","        \n","PATH = \"/kaggle/input/Hindi_English_Truncated_Corpus.csv\"\n"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess English and Hindi sentences"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:57.956672Z","iopub.status.busy":"2024-06-24T16:51:57.956367Z","iopub.status.idle":"2024-06-24T16:51:57.961400Z","shell.execute_reply":"2024-06-24T16:51:57.960412Z","shell.execute_reply.started":"2024-06-24T16:51:57.956646Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.15.0\n"]}],"source":["print(tf.__version__)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:58.192596Z","iopub.status.busy":"2024-06-24T16:51:58.192338Z","iopub.status.idle":"2024-06-24T16:51:58.198840Z","shell.execute_reply":"2024-06-24T16:51:58.197920Z","shell.execute_reply.started":"2024-06-24T16:51:58.192574Z"},"trusted":true},"outputs":[],"source":["def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    w = w.rstrip().strip()\n","    return w\n","\n","def hindi_preprocess_sentence(w):\n","    w = w.rstrip().strip()\n","    return w"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:58.404715Z","iopub.status.busy":"2024-06-24T16:51:58.404451Z","iopub.status.idle":"2024-06-24T16:51:58.411930Z","shell.execute_reply":"2024-06-24T16:51:58.410989Z","shell.execute_reply.started":"2024-06-24T16:51:58.404693Z"},"trusted":true},"outputs":[],"source":["def create_dataset(path=PATH):\n","    lines=pd.read_csv(path,encoding='utf-8')\n","    lines=lines.dropna()\n","    lines = lines[lines['source']=='ted']\n","    en = []\n","    hd = []\n","    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n","        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n","        en_1.append('<end>')\n","        en_1.insert(0, '<start>')\n","        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n","        hd_1.append('<end>')\n","        hd_1.insert(0, '<start>')\n","        en.append(en_1)\n","        hd.append(hd_1)\n","    return hd, en"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:58.580229Z","iopub.status.busy":"2024-06-24T16:51:58.579977Z","iopub.status.idle":"2024-06-24T16:51:58.584119Z","shell.execute_reply":"2024-06-24T16:51:58.583278Z","shell.execute_reply.started":"2024-06-24T16:51:58.580208Z"},"trusted":true},"outputs":[],"source":["def max_length(tensor):\n","    return max(len(t) for t in tensor)"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenization of the data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:58.966523Z","iopub.status.busy":"2024-06-24T16:51:58.965766Z","iopub.status.idle":"2024-06-24T16:51:58.971154Z","shell.execute_reply":"2024-06-24T16:51:58.970227Z","shell.execute_reply.started":"2024-06-24T16:51:58.966492Z"},"trusted":true},"outputs":[],"source":["def tokenize(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","  lang_tokenizer.fit_on_texts(lang)\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n","  return tensor, lang_tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:59.144141Z","iopub.status.busy":"2024-06-24T16:51:59.143226Z","iopub.status.idle":"2024-06-24T16:51:59.149871Z","shell.execute_reply":"2024-06-24T16:51:59.148989Z","shell.execute_reply.started":"2024-06-24T16:51:59.144104Z"},"trusted":true},"outputs":[],"source":["def load_dataset(path=PATH):\n","    targ_lang, inp_lang = create_dataset(path)\n","    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:51:59.350955Z","iopub.status.busy":"2024-06-24T16:51:59.350299Z","iopub.status.idle":"2024-06-24T16:52:05.629385Z","shell.execute_reply":"2024-06-24T16:52:05.628366Z","shell.execute_reply.started":"2024-06-24T16:51:59.350917Z"},"trusted":true},"outputs":[],"source":["input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n","max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"]},{"cell_type":"markdown","metadata":{},"source":["### Create Train and Test dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:52:05.631387Z","iopub.status.busy":"2024-06-24T16:52:05.631094Z","iopub.status.idle":"2024-06-24T16:52:05.650621Z","shell.execute_reply":"2024-06-24T16:52:05.649509Z","shell.execute_reply.started":"2024-06-24T16:52:05.631363Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["31904 31904 7977 7977\n"]}],"source":["input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:52:05.652180Z","iopub.status.busy":"2024-06-24T16:52:05.651831Z","iopub.status.idle":"2024-06-24T16:52:05.659290Z","shell.execute_reply":"2024-06-24T16:52:05.658435Z","shell.execute_reply.started":"2024-06-24T16:52:05.652150Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Language; index to word mapping\n","1 ----> <start>\n","44 ----> these\n","718 ----> aren t\n","1179 ----> financial\n","7808 ----> stats\n","16 ----> this\n","11 ----> is\n","1252 ----> culture .\n","2 ----> <end>\n","\n","Target Language; index to word mapping\n","1 ----> <start>\n","21 ----> ये\n","1562 ----> वित्तीय\n","24452 ----> आँकड़े\n","15 ----> नहीं\n","32 ----> हैं,\n","12 ----> यह\n","601 ----> संस्कृति\n","30 ----> है.\n","2 ----> <end>\n"]}],"source":["def convert(lang, tensor):\n","  for t in tensor:\n","    if t!=0:\n","      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n","    \n","print (\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print ()\n","print (\"Target Language; index to word mapping\")\n","convert(targ_lang, target_tensor_train[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T17:11:35.916856Z","iopub.status.busy":"2024-06-24T17:11:35.916489Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 Batch 0 Loss 2.9710\n","Epoch 1 Batch 100 Loss 2.0449\n","Epoch 1 Batch 200 Loss 1.9144\n","Epoch 1 Batch 300 Loss 1.7406\n","Epoch 1 Batch 400 Loss 1.8963\n","Epoch 1 Loss 1.9197\n","Time taken for 1 epoch 210.03436541557312 sec\n","\n","Epoch 2 Batch 0 Loss 1.6099\n","Epoch 2 Batch 100 Loss 1.7314\n","Epoch 2 Batch 200 Loss 1.6245\n","Epoch 2 Batch 300 Loss 1.7462\n","Epoch 2 Batch 400 Loss 1.6286\n","Epoch 2 Loss 1.6332\n","Time taken for 1 epoch 185.77496004104614 sec\n","\n","Epoch 3 Batch 0 Loss 1.6220\n","Epoch 3 Batch 100 Loss 1.6470\n","Epoch 3 Batch 200 Loss 1.6118\n","Epoch 3 Batch 300 Loss 1.5018\n","Epoch 3 Batch 400 Loss 1.5294\n","Epoch 3 Loss 1.5128\n","Time taken for 1 epoch 185.7384090423584 sec\n","\n","Epoch 4 Batch 0 Loss 1.3681\n","Epoch 4 Batch 100 Loss 1.3668\n","Epoch 4 Batch 200 Loss 1.3861\n","Epoch 4 Batch 300 Loss 1.4530\n","Epoch 4 Batch 400 Loss 1.5092\n","Epoch 4 Loss 1.4292\n","Time taken for 1 epoch 185.74898028373718 sec\n","\n","Epoch 5 Batch 0 Loss 1.3025\n","Epoch 5 Batch 100 Loss 1.2838\n","Epoch 5 Batch 200 Loss 1.3996\n","Epoch 5 Batch 300 Loss 1.4994\n","Epoch 5 Batch 400 Loss 1.1786\n","Epoch 5 Loss 1.3591\n","Time taken for 1 epoch 185.72853422164917 sec\n","\n","Epoch 6 Batch 0 Loss 1.2119\n","Epoch 6 Batch 100 Loss 1.1332\n","Epoch 6 Batch 200 Loss 1.1861\n","Epoch 6 Batch 300 Loss 1.2059\n","Epoch 6 Batch 400 Loss 1.3294\n","Epoch 6 Loss 1.2974\n","Time taken for 1 epoch 185.76681923866272 sec\n","\n","Epoch 7 Batch 0 Loss 1.1560\n","Epoch 7 Batch 100 Loss 1.2502\n","Epoch 7 Batch 200 Loss 1.1317\n","Epoch 7 Batch 300 Loss 1.3004\n","Epoch 7 Batch 400 Loss 1.3763\n","Epoch 7 Loss 1.2407\n","Time taken for 1 epoch 185.79256939888 sec\n","\n","Epoch 8 Batch 0 Loss 1.2198\n","Epoch 8 Batch 100 Loss 1.2061\n","Epoch 8 Batch 200 Loss 1.3008\n","Epoch 8 Batch 300 Loss 1.2007\n","Epoch 8 Batch 400 Loss 1.0757\n","Epoch 8 Loss 1.1886\n","Time taken for 1 epoch 185.7455644607544 sec\n","\n","Epoch 9 Batch 0 Loss 0.9813\n","Epoch 9 Batch 100 Loss 1.1447\n","Epoch 9 Batch 200 Loss 1.1735\n","Epoch 9 Batch 300 Loss 1.2501\n","Epoch 9 Batch 400 Loss 1.1607\n","Epoch 9 Loss 1.1388\n","Time taken for 1 epoch 185.772531747818 sec\n","\n","Epoch 10 Batch 0 Loss 1.1905\n","Epoch 10 Batch 100 Loss 1.0591\n","Epoch 10 Batch 200 Loss 1.0458\n"]}],"source":["import tensorflow as tf\n","import time\n","# Bahdanau Attention\n","class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","    def call(self, query, values):\n","        hidden_with_time_axis = tf.expand_dims(query, 1)\n","        score = self.V(tf.nn.tanh(\n","            self.W1(values) + self.W2(hidden_with_time_axis)))\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        return context_vector, attention_weights\n","# Encoder\n","class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n","                                          return_sequences=True,\n","                                          return_state=True)\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n","        return output, [state_h, state_c]\n","    def initialize_hidden_state(self):\n","        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n","# Decoder\n","class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n","                                          return_sequences=True,\n","                                          return_state=True)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","        self.attention = BahdanauAttention(self.dec_units)\n","    def call(self, x, hidden, enc_output):\n","        context_vector, attention_weights = self.attention(hidden[0], enc_output)\n","        x = self.embedding(x)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        output, state_h, state_c = self.lstm(x)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        x = self.fc(output)\n","        return x, [state_h, state_c], attention_weights\n","# Define hyperparameters\n","BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index) + 1\n","vocab_tar_size = len(targ_lang.word_index) + 1\n","# Create datasets\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","# Initialize encoder and decoder\n","encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","# Define optimizer and loss function\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","# Define loss function\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_mean(loss_)\n","# Define training step\n","@tf.function\n","def train_step(inp, targ, enc_hidden):\n","    loss = 0\n","    with tf.GradientTape() as tape:\n","        enc_output, enc_hidden = encoder(inp, enc_hidden)\n","        dec_hidden = enc_hidden\n","        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","        for t in range(1, targ.shape[1]):\n","            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","            loss += loss_function(targ[:, t], predictions)\n","            dec_input = tf.expand_dims(targ[:, t], 1)\n","    batch_loss = (loss / int(targ.shape[1]))\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","    return batch_loss\n","# Training loop\n","EPOCHS = 10\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","    enc_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        batch_loss = train_step(inp, targ, enc_hidden)\n","        total_loss += batch_loss\n","        if batch % 100 == 0:\n","            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n","    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n","    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Encoder"]},{"cell_type":"markdown","metadata":{},"source":["### Optimizer"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:55:38.860282Z","iopub.status.busy":"2024-06-24T16:55:38.860019Z","iopub.status.idle":"2024-06-24T16:55:38.869121Z","shell.execute_reply":"2024-06-24T16:55:38.868157Z","shell.execute_reply.started":"2024-06-24T16:55:38.860260Z"},"trusted":true},"outputs":[],"source":["def evaluate(sentence):\n","    attention_plot = np.zeros((max_length_targ, max_length_inp))\n","    sentence = preprocess_sentence(sentence)\n","    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                           maxlen=max_length_inp,\n","                                                     padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    result = ''\n","    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]  # Initialize with cell state as well\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                             dec_hidden,\n","                                                             enc_out)\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        result += targ_lang.index_word[predicted_id] + ' '\n","        if targ_lang.index_word[predicted_id] == '<end>':\n","            return result, sentence\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","    return result, sentence\n","print(\"done\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:55:38.870760Z","iopub.status.busy":"2024-06-24T16:55:38.870398Z","iopub.status.idle":"2024-06-24T16:55:38.883565Z","shell.execute_reply":"2024-06-24T16:55:38.882732Z","shell.execute_reply.started":"2024-06-24T16:55:38.870710Z"},"trusted":true},"outputs":[],"source":["def translate(sentence):\n","    result, sentence = evaluate(sentence)\n","    print('Input: %s' % (sentence))\n","    print('Predicted translation: {}'.format(result))\n","print(\"done\")\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:59:03.374809Z","iopub.status.busy":"2024-06-24T16:59:03.373966Z","iopub.status.idle":"2024-06-24T16:59:03.773873Z","shell.execute_reply":"2024-06-24T16:59:03.773086Z","shell.execute_reply.started":"2024-06-24T16:59:03.374768Z"},"trusted":true},"outputs":[],"source":["encoder.save('encoder_model.h5')\n","decoder.save('decoder_model.h5')\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T16:56:11.517974Z","iopub.status.busy":"2024-06-24T16:56:11.517511Z","iopub.status.idle":"2024-06-24T16:56:11.746754Z","shell.execute_reply":"2024-06-24T16:56:11.745843Z","shell.execute_reply.started":"2024-06-24T16:56:11.517928Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input: politics are not for kids\n","Predicted translation: ये एक तरह <end> \n"]}],"source":["translate(u'politics are not for kids')"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T17:02:30.495850Z","iopub.status.busy":"2024-06-24T17:02:30.494910Z","iopub.status.idle":"2024-06-24T17:02:30.502264Z","shell.execute_reply":"2024-06-24T17:02:30.501427Z","shell.execute_reply.started":"2024-06-24T17:02:30.495819Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/decoder_model.h5'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["import shutil\n","\n","shutil.move('encoder_model.h5', '/kaggle/working/encoder_model.h5')\n","shutil.move('decoder_model.h5', '/kaggle/working/decoder_model.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":200079,"sourceId":441417,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
